---
title: "Predicting Property Values"
author: "Anthony J Smith"
format: html
editor: visual
---

```{r message=FALSE}
library(tidyverse)
library(tidymodels)
library(kableExtra)
library(magrittr)
library(dplyr)
library(ggplot2)
library(patchwork)
library(tidytext)
library(dials)
library(tensorflow)
library(reticulate)
```

```{r message=FALSE}
zillow_data <- read_csv("E:/MAT434/snhu-mat434-zillow-classification-challenge/data.csv")
names(zillow_data) <- janitor::make_clean_names(names(zillow_data))

# add new data to perform predictions on based on learning from previous data
comp <- read_csv("E:/MAT434/comp_csv/comp.csv")
names(comp) <- janitor::make_clean_names(names(comp))
```

## Statement of Purpose

Real estate prices in Austin Texas have skyrocketed over recent years. This analysis explores data from Austin's market to develop an understanding of what features contribute to a properties value. Knowledge of this can be used predict a property's value based on it's features.

## Exploratory Analysis

The data contains `r zillow_data %>% nrow()` rows of observations with `r zillow_data %>% ncol()` variables. These variables are:

```{r}
# convert column names to a dataframe
colnames_df <- data.frame((colnames(zillow_data)))
# Rename the header
colnames(colnames_df)[1] <- "Variables"


# display in table
colnames_df %>%
  knitr::kable() %>%
  kableExtra::kable_styling(bootstrap_options = c("hover", "striped"))
```

A visual map of where these properties are located and what they're valued at can develop an understanding on what contributes to a properties value.

```{r}
zillow_data %>%
  ggplot(aes(longitude, latitude, color = price_range)) +
  geom_point(size = 0.5, alpha = 0.4) +
   guides(color = guide_legend(override.aes = list(size=3))) +
  labs(
    color = NULL,
    title = "Location of Properties in Austin Texas by Price Range"
       )
```

From here, we can see that the first few properties in the data set are the following:

```{r}
zillow_data %>%
  head() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```

```{r}
counts <- table(zillow_data$home_type)
```

Supply and demand is a common way of determining property1 value, however we are currently unaware of what the *supply* is nor where the *demand* may be. There are `r counts['Single Family']` single family homes, `r counts['MultiFamily']` multifamily homes, `r counts['Condo']` condos, `r counts['Townhouse']` town houses, `r counts ['Mobile / Manufactured']` manufactured homes, `r counts ['Multiple Occupancy']` multiple occupancy's, and `r counts['Residential']` residential homes. A visual representation of this is shown below.

```{r}
options(repr.plot.width=25, repr.plot.height=27)

# Create a data frame with the counts
homeType_barchart <- data.frame(
  home_type = names(counts),
  count = as.numeric(counts)
)

# Reorder the factor levels based on the count
homeType_barchart$home_type <- reorder(homeType_barchart$home_type, homeType_barchart$count)

# Create a bar chart
ggplot(homeType_barchart, aes(x = home_type, y = count, fill = home_type)) +
  geom_bar(stat = "identity") +
  labs(title = "Distribution of Different Home Types") +
  ylab("Number of Property Types") +
  xlab("Count") +
  
  coord_flip()
```

Above, we can see that Single Family homes take up a significantly large portion of the market. Below we display the same data, but this time removing Single Family homes for visual aid.

```{r}
options(repr.plot.width=25, repr.plot.height=27)

# Create a data frame with the counts
homeType_barchart <- data.frame(
  home_type = names(counts),
  count = as.numeric(counts)
)

# Filter out "Single Family"
homeType_barchart <- homeType_barchart[homeType_barchart$home_type != "Single Family",]
```

## Building Models & Making Predictions

There are a lot of models out there that can be used for predicting the value of a these properties, however some will be better than others at making predictions. We will now explore different models and asses how they perform at predicting property values with our data set. To do this, we'll need split our data into training and test sets. Splitting the data allows us to create separate environments with the data where one portion is used for training our models on making predictions, and the latter is used for testing the accuracy of our models at making predictions. We'll begin with constructing a Decision Tree model.

```{r}
set.seed(434)

data_for_model <- zillow_data %>%
  mutate(has_spa = as.integer(has_spa))

zillow_data_split <- initial_split(data_for_model, prop = 0.75)
zillow_training <- training(zillow_data_split)
zillow_test <- testing(zillow_data_split)

# create cross validation folds for multiple sample tests
train_folds <- vfold_cv(zillow_training, v = 10)
```

## Using Decision Trees
```{r}
#Create a model specification
dt_spec <- decision_tree(
  tree_depth = tune()
  ) %>%
  set_engine("rpart") %>%
  set_mode("classification") 

grid_depth <- tibble("tree_depth" = c(2, 3, 4, 5, 8, 10, 12, 15, 20))

#Create a recipe
dt_rec <- recipe(price_range ~ ., data = zillow_training) %>%
  step_rm(description) %>%
  step_impute_median(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())

dt_wf <- workflow() %>%
  add_model(dt_spec) %>% 
  add_recipe(dt_rec)

dt_tune_results <- dt_wf %>%
  tune_grid(
    grid = grid_depth,
    resamples = train_folds
  )

dt_tune_results %>%
  collect_metrics() %>%
  arrange(-mean) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))

```

Rebuild model using best performing metrics and fit to training data
```{r}
dt_clf_spec <- decision_tree(
  tree_depth = 5 # setting grid depth to 5 based on performance results
  ) %>% 
  set_engine("rpart") %>%
  set_mode("classification") 

dt_rec <- recipe( price_range ~ ., data = zillow_training) %>%
  step_rm(description) %>%
  step_impute_median(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())

dt_wf <- workflow() %>%
  add_model(dt_clf_spec) %>%
  add_recipe(dt_rec)

dt_fit <- dt_wf %>%
  fit(zillow_training)


dt_fit %>% # Predictions with new data set
  augment(comp)

submission_df <- dt_fit %>%
  augment(comp) %>%
  select(id, contains(".pred"), -.pred_class) %>%
  rename(prob_A = ".pred_0-250000",
         prob_B = ".pred_250000-350000",
         prob_C = ".pred_350000-450000",
         prob_D = ".pred_450000-650000",
         prob_E = ".pred_650000+")

write.csv(submission_df, "decision_tree.csv", row.names = FALSE)
```

## KNN Model Construction and Interpretation
```{r}
#knn workflow
knn_spec <- nearest_neighbor() %>%
  set_engine("kknn") %>%
  set_mode("classification")

# building recipe
knn_rec <- recipe(price_range ~ lot_size_sq_ft + num_of_bedrooms + num_of_bathrooms + avg_school_rating, data = zillow_training) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())

knn_wf <- workflow() %>%
  add_model(knn_spec) %>%
  add_recipe(knn_rec)

knn_cv_results <- knn_wf %>%
  fit_resamples(train_folds)

knn_cv_results %>%  
  collect_metrics() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```

## Random Forest Classifier
```{r}
train_folds <- vfold_cv(zillow_training, v = 10)

rf_spec <- rand_forest(
  trees = tune(),
  mtry = tune(),
  min_n = tune()
) %>%
  set_engine("ranger") %>%
  set_mode("classification")

grid_depth <- tibble( # 'crossing':Crossing intertwines settings
  "trees" = c(5000, 2000, 4000), 
  "mtry" = c(10, 3, 5),
  "min_n" = c(10, 15, 20)
)

rf_rec <- recipe( price_range ~ ., data = zillow_training) %>%
  step_rm(description) %>%
  step_impute_median(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())


rf_wf <- workflow() %>%
  add_recipe(rf_rec) %>%
  add_model(rf_spec)

rf_tune_results <- rf_wf %>%
  tune_grid(     
    resamples = train_folds,
    grid = grid_depth
  )

rf_tune_results %>%
  collect_metrics()
```

Select the best Random Forest model and fit to training data
```{r}
best_params <- rf_tune_results %>%
               select_best("accuracy")  # using 'accuracy' as desired metric
best_params

rf_spec_final <- finalize_workflow(rf_wf, best_params)

rf_fit <- fit(rf_spec_final, data = zillow_training)

augmented_data <- rf_fit %>%
  augment(comp)

submission_df <- rf_fit %>%
  augment(comp) %>%
  select(id, contains(".pred"), -.pred_class) %>%
  rename(prob_A = ".pred_0-250000",
         prob_B = ".pred_250000-350000",
         prob_C = ".pred_350000-450000",
         prob_D = ".pred_450000-650000",
         prob_E = ".pred_650000+")

write.csv(submission_df, "rand_forest.csv", row.names = FALSE)
```

## Boosted Model
```{r}
train_folds <- vfold_cv(zillow_training, v = 10)

# What we're going to tune
boost_spec <- boost_tree(
  trees = tune(), # boosting iterations
  min_n = tune(), # determine minimum number of elements in group for further splitting
  learn_rate = tune()
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# The tuning
grid_depth <- tibble( # 'crossing':Crossing intertwines settings
  "trees" =      c(19, 20, 21, 22, 23, 24, 25), 
  "min_n" =      c(17, 18, 19, 20, 21, 22, 23),
  "learn_rate" = c(0.43, 0.44, 0.45, 0.44,0.46, 0.47, 0.48)
)

boost_rec <- recipe( price_range ~ ., data = zillow_training) %>%
  step_rm(description) %>%
  step_impute_median(all_numeric_predictors()) %>%  
  step_impute_mode(all_nominal_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) 

boost_wf <- workflow() %>%
  add_model(boost_spec) %>%
  add_recipe(boost_rec)

boost_results <- boost_wf %>%
  tune_grid( # this tune_grid will randomly tune the values 'trees', 'min_n', & 'learn_rate'
    grid = grid_depth, # here we've chosen to perform random tuning 3 times
    resamples = train_folds # training folds was selected above to be 10
  )

boost_results %>%
  collect_metrics() # display results from the tuning iterations
```

Select the best Boosted model and fit to training data
```{r}
best_boost_params <- boost_results %>%
               select_best("accuracy")  # using 'accuracy' as desired metric

boost_spec_final <- finalize_workflow(boost_wf, best_boost_params)

boost_fit <- fit(boost_spec_final, data = zillow_training)

boost_predictions_test <- boost_fit %>%
  augment(zillow_test)

boost_fit %>%
  augment(zillow_test) 

augmented_data <- boost_fit %>%
  augment(comp)

  
submission_df <- boost_fit %>%
  augment(comp) %>%
  select(id, contains(".pred"), -.pred_class) %>%
  rename(prob_A = ".pred_0-250000",
         prob_B = ".pred_250000-350000",
         prob_C = ".pred_350000-450000",
         prob_D = ".pred_450000-650000",
         prob_E = ".pred_650000+")


write.csv(submission_df, "boosted_model.csv", row.names = FALSE)
```

## Exploring Text and Tokenization (the importance of the description in home value)
```{r echo = FALSE}
tokenized_openers <- zillow_data %>%
  unnest_tokens(word, description) %>%
  anti_join(stop_words, by = "word")

tokenized_openers %>%
  head() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```

```{r}
# TODO this code is Imported Code from a Dr. G lesson 
zillow_training %>%
  unnest_tokens(word, description) %>%
  anti_join(stop_words) %>%
  filter(!(word %in% c("home", "bedrooms", "bathrooms", "kitchen", "family", "austin", "living", "master", "built", "backyard"))) %>%
  filter(!str_detect(word, "\\d+")) %>%
  group_by(priceRange) %>%
  count(word) %>%
  top_n(15) %>%
  ungroup() %>%
  mutate(priceRange = as.factor(priceRange),
         word = reorder_within(word, n, priceRange)) %>%
  ggplot() +
  geom_col(aes(x = word, y = n, fill = priceRange), show.legend = FALSE) + 
  scale_x_reordered() +
  coord_flip() +
  facet_wrap(~priceRange, scales = "free")
```
