---
title: "Predicting Property Values"
author: "Anthony J Smith"
format: html
editor: visual
---

```{r message=FALSE}
#Libraries 
library(tidyverse)
library(tidymodels)
library(kableExtra)
library(magrittr)
library(dplyr)
library(ggplot2)
library(patchwork)
library(tidytext)
library(dials)

```

```{r message=FALSE}
zillow_data <- read_csv("E:/MAT434/snhu-mat434-zillow-classification-challenge/data.csv")
names(zillow_data) <- janitor::make_clean_names(names(zillow_data))

# add new data to perform predictions on based on learning from previous data
comp <- read_csv("E:/MAT434/comp_csv/comp.csv")
names(comp) <- janitor::make_clean_names(names(comp))
```

## Statement of Purpose

Real estate prices in Austin Texas have skyrocketed over recent years. This analysis explores data from Austin's market to develop an understanding of what features contribute to a properties value. Knowledge of this can be used predict a property's value based on it's features.

## Exploratory Analysis

The data contains `r zillow_data %>% nrow()` rows of observations with `r zillow_data %>% ncol()` variables. These variables are:

```{r}
# convert column names to a dataframe
colnames_df <- data.frame((colnames(zillow_data)))
# Rename the header
colnames(colnames_df)[1] <- "Variables"


# display in table
colnames_df %>%
  knitr::kable() %>%
  kableExtra::kable_styling(bootstrap_options = c("hover", "striped"))
```
A visual map of where these properties are located and what they're valued at can develop an understanding on what  contributes to a properties value.
```{r}
zillow_data %>%
  ggplot(aes(longitude, latitude, color = price_range)) +
  geom_point(size = 0.5, alpha = 0.4) +
   guides(color = guide_legend(override.aes = list(size=3))) +
  labs(
    color = NULL,
    title = "Location of Properties in Austin Texas by Price Range"
       )
```

From here, we can see that the first few properties in the data set are the following:

```{r}
zillow_data %>%
  head() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```

```{r}
counts <- table(zillow_data$home_type)
```

Supply and demand is a common way of determining property1 value, however we are currently unaware of what the *supply* is nor where the *demand* may be. There are `r counts['Single Family']` single family homes, `r counts['MultiFamily']` multifamily homes, `r counts['Condo']` condos, `r counts['Townhouse']` town houses, `r counts ['Mobile / Manufactured']` manufactured homes, `r counts ['Multiple Occupancy']` multiple occupancy's, and `r counts['Residential']` residential homes. A visual representation of this is shown below.

```{r}
options(repr.plot.width=25, repr.plot.height=27)

# Create a data frame with the counts
homeType_barchart <- data.frame(
  home_type = names(counts),
  count = as.numeric(counts)
)

# Reorder the factor levels based on the count
homeType_barchart$home_type <- reorder(homeType_barchart$home_type, homeType_barchart$count)

# Create a bar chart
ggplot(homeType_barchart, aes(x = home_type, y = count, fill = home_type)) +
  geom_bar(stat = "identity") +
  labs(title = "Distribution of Different Home Types") +
  ylab("Number of Property Types") +
  xlab("Count") +
  
  coord_flip()
```

Above, we can see that Single Family homes take up a significantly large portion of the market. Below we display the same data, but this time removing Single Family homes for visual aid.

```{r}
options(repr.plot.width=25, repr.plot.height=27)

# Create a data frame with the counts
homeType_barchart <- data.frame(
  home_type = names(counts),
  count = as.numeric(counts)
)

# Filter out "Single Family"
homeType_barchart <- homeType_barchart[homeType_barchart$home_type != "Single Family",]
```

## Building Models & Making Predictions

There are a lot of models out there that can be used for predicting the value of a these properties, however some will be better than others at making predictions. We will now explore different models and asses how they perform at predicting property values with our data set. To do this, we'll need split our data into training and test sets. Splitting the data allows us to create separate environments with the data where one portion is used for training our models on making predictions, and the latter is used for testing the accuracy of our models at making predictions. We'll begin with constructing a Decision Tree model.

```{r}
set.seed(434)

data_for_model <- zillow_data %>%
  mutate(has_spa = as.integer(has_spa))

zillow_data_split <- initial_split(data_for_model, prop = 0.75)
zillow_training <- training(zillow_data_split)
zillow_test <- testing(zillow_data_split)

# create cross validation folds for multiple sample tests
train_folds <- vfold_cv(zillow_training, v = 10)
```

## Using Decision Trees
```{r}
#Create a model specification
dt_spec <- decision_tree(tree_depth = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification") 

grid_depth <- tibble(
  "tree_depth" = c(2, 3, 4, 5, 8, 10, 12, 15, 20))

#Create a recipe
dt_rec <- recipe(price_range ~ ., data = zillow_training) %>%
  step_rm(description) %>%
  step_impute_median(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())

dt_wf <- workflow() %>%
  add_model(dt_spec) %>% 
  add_recipe(dt_rec)

#metrics <- metric_set(accuracy, m_log_loss)

dt_tune_results <- dt_wf %>%
  tune_grid(
    grid = grid_depth,
    resamples = train_folds
  )

dt_tune_results %>%
  collect_metrics() %>%
  arrange(-mean) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
# 
# ```
# 
# ```{r}
dt_clf_spec <- decision_tree(tree_depth = 5) %>%
  set_engine("rpart") %>%
  set_mode("classification") 

dt_rec <- recipe( price_range ~ ., data = zillow_training) %>%
  step_rm(description) %>%
  step_impute_median(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())

dt_wf <- workflow() %>%
  add_model(dt_clf_spec) %>%
  add_recipe(dt_rec)

dt_fit <- dt_wf %>%
  fit(zillow_training)

# Predictions with new dataset
dt_fit %>%
  augment(comp)

submission_df <- dt_fit %>%
  augment(comp) %>%
  select(id, contains(".pred"), -.pred_class) %>%
  rename(prob_A = ".pred_0-250000",
         prob_B = ".pred_250000-350000",
         prob_C = ".pred_350000-450000",
         prob_D = ".pred_450000-650000",
         prob_E = ".pred_650000+")

write.csv(submission_df, "submission.csv", row.names = FALSE)
```

## KNN Model Construction and Interpretation

```{r}
#knn workflow
knn_spec <- nearest_neighbor() %>%
  set_engine("kknn") %>%
  set_mode("classification")

# building recipe
knn_rec <- recipe(price_range ~ lot_size_sq_ft + num_of_bedrooms + num_of_bathrooms + avg_school_rating, data = zillow_training) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())

knn_wf <- workflow() %>%
  add_model(knn_spec) %>%
  add_recipe(knn_rec)

knn_cv_results <- knn_wf %>%
  fit_resamples(train_folds)

knn_cv_results %>%  
  collect_metrics() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```


## Random Forest Classifier
```{r}
grid_depth <- tibble("tree_depth" = c(2, 3, 4, 5, 8, 10, 12, 15, 20))

rf_spec <- rand_forest(
  mtry = tune(),
  trees = 100,
  min_n = tune(),
) %>%
  set_engine("ranger") %>%
  set_mode("classification")

rf_rec <- recipe( price_range ~ ., data = zillow_training) %>%
  step_rm(description) %>%
  step_impute_median(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())

rf_wf <- workflow() %>%
  add_recipe(rf_rec) %>%
  add_model(rf_spec)

rf_tune_results <- rf_wf %>%
  tune_grid(     
    resamples = train_folds
  )

tuned_metrics <- rf_tune_results %>%
  collect_metrics()
print(tuned_metrics)

rf_fit <- rf_workflow %>%
  fit(zillow_training)

# Predictions with new dataset
rf_fit %>%
  augment(comp)

submission_df <- rf_fit %>%
  augment(comp) %>%
  select(id, contains(".pred"), -.pred_class) %>%
  rename(prob_A = ".pred_0-250000",
         prob_B = ".pred_250000-350000",
         prob_C = ".pred_350000-450000",
         prob_D = ".pred_450000-650000",
         prob_E = ".pred_650000+")

write.csv(submission_df, "rand_forest.csv", row.names = FALSE)
```

## Boosted Model
```{r}
train_folds <- vfold_cv(zillow_training, v = 10)

# What we're going to tune
boost_spec <- boost_tree(
  mode = "classification",
  trees = 100, # boosting iterations
  min_n = 2, # determine minimum number of elements in group for further splitting
  learn_rate = 0.1
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# grid_depth <- tibble(
#   "tree_depth" = c(2, 3, 4, 5, 8, 10, 12, 15, 20))

boost_rec <- recipe( price_range ~ ., data = zillow_training) %>%
  step_rm(description) %>%
  step_impute_median(all_numeric_predictors()) %>%  
  step_impute_mode(all_nominal_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) 

boost_wf <- workflow() %>%
  add_model(boost_spec) %>%
  add_recipe(boost_rec)

boost_results <- boost_wf %>%
  fit_resamples(train_folds)

boost_fit <- boost_wf %>%
  fit(zillow_training)

boost_fit %>%
  augment(zillow_test) 

augmented_data <- boost_fit %>%
  augment(comp)

  
submission_df <- boost_fit %>%
  augment(comp) %>%
  select(id, contains(".pred"), -.pred_class) %>%
  rename(prob_A = ".pred_0-250000",
         prob_B = ".pred_250000-350000",
         prob_C = ".pred_350000-450000",
         prob_D = ".pred_450000-650000",
         prob_E = ".pred_650000+")


write.csv(submission_df, "boosted_zillow.csv", row.names = FALSE)
```

## Exploring Text and Tokenization (the importance of the description in home value)
```{r echo = FALSE}
tokenized_openers <- zillow_data %>%
  unnest_tokens(word, description) %>%
  anti_join(stop_words, by = "word")

tokenized_openers %>%
  head() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```
```{r} 
# TODO this code is Imported Code from a Dr. G lesson 
zillow_training %>%
  unnest_tokens(word, description) %>%
  anti_join(stop_words) %>%
  filter(!(word %in% c("home", "bedrooms", "bathrooms", "kitchen", "family", "austin", "living", "master", "built", "backyard"))) %>%
  filter(!str_detect(word, "\\d+")) %>%
  group_by(priceRange) %>%
  count(word) %>%
  top_n(15) %>%
  ungroup() %>%
  mutate(priceRange = as.factor(priceRange),
         word = reorder_within(word, n, priceRange)) %>%
  ggplot() +
  geom_col(aes(x = word, y = n, fill = priceRange), show.legend = FALSE) + 
  scale_x_reordered() +
  coord_flip() +
  facet_wrap(~priceRange, scales = "free")
```